"""
ğŸ­ ì‚°ì—…ì•ˆì „ë³´ê±´ë²• AI ìƒë‹´ì‚¬
Streamlit ì›¹ ì•± (API í‚¤ ë‚´ì¥ ë²„ì „)
"""

import streamlit as st
import requests
import xml.etree.ElementTree as ET
import chromadb
from sentence_transformers import SentenceTransformer
from anthropic import Anthropic

# ============================================================
# í˜ì´ì§€ ì„¤ì •
# ============================================================
st.set_page_config(
    page_title="ì‚°ì—…ì•ˆì „ë³´ê±´ë²• AI ìƒë‹´ì‚¬",
    page_icon="ğŸ­",
    layout="centered"
)

# ============================================================
# ğŸ” API í‚¤ ì„¤ì • (Streamlit Secretsì—ì„œ ê°€ì ¸ì˜´)
# ============================================================
# Streamlit Cloudì˜ Secretsì— ì €ì¥ëœ API í‚¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤
# ì„¤ì • ë°©ë²•: Streamlit Cloud â†’ ì•± Settings â†’ Secrets
ANTHROPIC_API_KEY = st.secrets["ANTHROPIC_API_KEY"]

# ============================================================
# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
# ============================================================
if 'messages' not in st.session_state:
    st.session_state.messages = []

# ============================================================
# ë²•ë ¹ ë°ì´í„° ë¡œë“œ í•¨ìˆ˜ë“¤
# ============================================================
@st.cache_resource
def load_embedding_model():
    """ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (ìºì‹œë¨)"""
    return SentenceTransformer('jhgan/ko-sroberta-multitask')

@st.cache_data
def get_law_articles(law_msn, oc="kangyoon.kim"):
    """ë²•ë ¹ ì¡°ë¬¸ ê°€ì ¸ì˜¤ê¸° (ìºì‹œë¨)"""
    url = "http://www.law.go.kr/DRF/lawService.do"
    params = {
        "OC": oc,
        "target": "law",
        "type": "XML",
        "MST": law_msn
    }
    
    response = requests.get(url, params=params)
    response.encoding = 'utf-8'
    root = ET.fromstring(response.content)
    
    articles = []
    
    for article in root.findall('.//ì¡°ë¬¸ë‹¨ìœ„'):
        article_no = article.findtext('ì¡°ë¬¸ë²ˆí˜¸', '')
        article_title = article.findtext('ì¡°ë¬¸ì œëª©', '')
        article_content = article.findtext('ì¡°ë¬¸ë‚´ìš©', '')
        
        hang_list = []
        for hang in article.findall('.//í•­'):
            hang_content = hang.findtext('í•­ë‚´ìš©', '')
            if hang_content:
                hang_list.append(hang_content)
        
        full_text = f"ì œ{article_no}ì¡°"
        if article_title:
            full_text += f"({article_title})"
        full_text += "\n"
        if article_content:
            full_text += article_content + "\n"
        if hang_list:
            full_text += "\n".join(hang_list)
        
        if article_content or hang_list:
            articles.append({
                "article_no": article_no,
                "title": article_title,
                "full_text": full_text.strip()
            })
    
    return articles

@st.cache_resource
def build_vector_db(_embedding_model, articles):
    """ë²¡í„° DB êµ¬ì¶• (ìºì‹œë¨)"""
    chroma_client = chromadb.Client()
    
    try:
        chroma_client.delete_collection("osh_law")
    except:
        pass
    
    collection = chroma_client.create_collection(name="osh_law")
    
    for idx, article in enumerate(articles):
        text = article['full_text']
        embedding = _embedding_model.encode(text).tolist()
        
        collection.add(
            documents=[text],
            embeddings=[embedding],
            metadatas=[{"article_no": article['article_no'], "title": article['title'] or ""}],
            ids=[f"article_{idx}"]
        )
    
    return collection

def search_law(query, collection, embedding_model, n_results=3):
    """ê´€ë ¨ ì¡°ë¬¸ ê²€ìƒ‰"""
    query_embedding = embedding_model.encode(query).tolist()
    
    results = collection.query(
        query_embeddings=[query_embedding],
        n_results=n_results
    )
    
    return results

def ask_chatbot(question, collection, embedding_model):
    """ì±—ë´‡ ì§ˆë¬¸-ë‹µë³€"""
    # ê´€ë ¨ ì¡°ë¬¸ ê²€ìƒ‰
    search_results = search_law(question, collection, embedding_model)
    context = "\n\n---\n\n".join(search_results['documents'][0])
    
    # Claude API í˜¸ì¶œ
    client = Anthropic(api_key=ANTHROPIC_API_KEY)
    
    prompt = f"""ë‹¹ì‹ ì€ ì‚°ì—…ì•ˆì „ë³´ê±´ë²• ì „ë¬¸ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤.
ì•„ë˜ ì œê³µëœ ë²•ë ¹ ì¡°ë¬¸ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

## ì°¸ê³  ë²•ë ¹ ì¡°ë¬¸:
{context}

## ì§ˆë¬¸:
{question}

## ë‹µë³€ ì§€ì¹¨:
1. ë°˜ë“œì‹œ ìœ„ ì¡°ë¬¸ ë‚´ìš©ì„ ê·¼ê±°ë¡œ ë‹µë³€í•˜ì„¸ìš”
2. ê´€ë ¨ ì¡°ë¬¸ ë²ˆí˜¸ë¥¼ ëª…ì‹œí•˜ì„¸ìš” (ì˜ˆ: ì œ29ì¡°ì— ë”°ë¥´ë©´...)
3. ì¡°ë¬¸ì— ì—†ëŠ” ë‚´ìš©ì€ "í•´ë‹¹ ë‚´ìš©ì€ ì œê³µëœ ì¡°ë¬¸ì—ì„œ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤"ë¼ê³  ë‹µí•˜ì„¸ìš”
4. ì‰½ê³  ì¹œì ˆí•˜ê²Œ ì„¤ëª…í•˜ì„¸ìš”
5. ë§ˆì§€ë§‰ì— ë©´ì±…ì¡°í•­ì„ ì¶”ê°€í•˜ì„¸ìš”: "â€» ë³¸ ë‹µë³€ì€ ì°¸ê³ ìš©ì´ë©°, ì •í™•í•œ ë²•ë¥  í•´ì„ì€ ì „ë¬¸ê°€ì™€ ìƒë‹´í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤."

## ë‹µë³€:"""

    response = client.messages.create(
        model="claude-sonnet-4-20250514",
        max_tokens=1024,
        messages=[{"role": "user", "content": prompt}]
    )
    
    return response.content[0].text, search_results['documents'][0]

# ============================================================
# ë©”ì¸ UI
# ============================================================
st.title("ğŸ­ ì‚°ì—…ì•ˆì „ë³´ê±´ë²• AI ìƒë‹´ì‚¬")
st.markdown("ì‚°ì—…ì•ˆì „ë³´ê±´ë²•ì— ëŒ€í•´ ë¬´ì—‡ì´ë“  ì§ˆë¬¸í•˜ì„¸ìš”!")

# ì‚¬ì´ë“œë°”
with st.sidebar:
    st.header("ğŸ“š ì§€ì› ë²•ë ¹")
    st.markdown("- âœ… ì‚°ì—…ì•ˆì „ë³´ê±´ë²•")
    st.markdown("- ğŸ”œ í™”í•™ë¬¼ì§ˆê´€ë¦¬ë²• (ì¤€ë¹„ì¤‘)")
    st.markdown("- ğŸ”œ ì¤‘ëŒ€ì¬í•´ì²˜ë²Œë²• (ì¤€ë¹„ì¤‘)")
    
    st.markdown("---")
    
    st.header("ğŸ’¡ ì§ˆë¬¸ ì˜ˆì‹œ")
    st.markdown("""
    - ì‚¬ì—…ì£¼ì˜ ì•ˆì „ë³´ê±´êµìœ¡ ì˜ë¬´ëŠ”?
    - ì•ˆì „ê´€ë¦¬ì ì„ ì„ ê¸°ì¤€ì€?
    - ë„ê¸‰ì¸ì˜ ì•ˆì „ì¡°ì¹˜ ì˜ë¬´ëŠ”?
    - ì‚°ì—…ì¬í•´ ë³´ê³  ì˜ë¬´ëŠ”?
    """)
    
    st.markdown("---")
    
    st.header("â„¹ï¸ ì•ˆë‚´")
    st.markdown("""
    **ë©´ì±…ì¡°í•­**  
    ë³¸ ì„œë¹„ìŠ¤ëŠ” ì°¸ê³ ìš©ì´ë©°, 
    ë²•ë¥ ì  íš¨ë ¥ì´ ì—†ìŠµë‹ˆë‹¤.
    ì •í™•í•œ ë²•ë¥  í•´ì„ì€ 
    ì „ë¬¸ê°€ì™€ ìƒë‹´í•˜ì„¸ìš”.
    """)
    
    st.markdown("---")
    st.markdown("Made with â¤ï¸ by íìŠ¤")

# ì‹œìŠ¤í…œ ì´ˆê¸°í™”
with st.spinner("ğŸ”„ ì‹œìŠ¤í…œ ì¤€ë¹„ ì¤‘..."):
    embedding_model = load_embedding_model()
    articles = get_law_articles("276853")
    collection = build_vector_db(embedding_model, articles)

st.success(f"âœ… ì¤€ë¹„ ì™„ë£Œ! ({len(articles)}ê°œ ì¡°ë¬¸ ë¡œë“œ)")

# ì±„íŒ… íˆìŠ¤í† ë¦¬ í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ì‚¬ìš©ì ì…ë ¥
if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
    # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # ë‹µë³€ ìƒì„±
    with st.chat_message("assistant"):
        with st.spinner("ğŸ” ê´€ë ¨ ì¡°ë¬¸ ê²€ìƒ‰ ì¤‘..."):
            try:
                answer, references = ask_chatbot(prompt, collection, embedding_model)
                st.markdown(answer)
                
                # ì°¸ê³  ì¡°ë¬¸ í‘œì‹œ
                with st.expander("ğŸ“œ ì°¸ê³ í•œ ë²•ë ¹ ì¡°ë¬¸ ë³´ê¸°"):
                    for i, ref in enumerate(references, 1):
                        st.markdown(f"**ì¡°ë¬¸ {i}**")
                        st.text(ref[:500] + "..." if len(ref) > 500 else ref)
                        st.markdown("---")
                
                st.session_state.messages.append({"role": "assistant", "content": answer})
                
            except Exception as e:
                st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
