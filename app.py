"""
ğŸ­ ì•ˆì „í™˜ê²½ ë²•ê·œ AI ìƒë‹´ì‚¬
í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ë²„ì „ (ë²¡í„° + í‚¤ì›Œë“œ)
"""

__import__('pysqlite3')
import sys
sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')

import streamlit as st
import pickle
import gzip
import chromadb
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer
from anthropic import Anthropic

# ============================================================
# í˜ì´ì§€ ì„¤ì •
# ============================================================
st.set_page_config(
    page_title="ì•ˆì „í™˜ê²½ ë²•ê·œ AI ìƒë‹´ì‚¬",
    page_icon="ğŸ­",
    layout="centered"
)

# ============================================================
# ë‚˜ëˆ”ê³ ë”• í°íŠ¸
# ============================================================
st.markdown("""
<style>
@import url('https://fonts.googleapis.com/css2?family=Nanum+Gothic:wght@400;700;800&display=swap');
html, body, [class*="css"] { font-family: 'Nanum Gothic', sans-serif; }
h1, h2, h3 { font-family: 'Nanum Gothic', sans-serif; font-weight: 800; }
</style>
""", unsafe_allow_html=True)

# ============================================================
# API í‚¤
# ============================================================
ANTHROPIC_API_KEY = st.secrets["ANTHROPIC_API_KEY"]

# ============================================================
# ì„¸ì…˜ ìƒíƒœ
# ============================================================
if 'messages' not in st.session_state:
    st.session_state.messages = []

# ============================================================
# ë°ì´í„° ë¡œë“œ (ìºì‹œë¨)
# ============================================================

@st.cache_resource
def load_embedding_model():
    return SentenceTransformer('jhgan/ko-sroberta-multitask')

@st.cache_resource
def load_data_and_build_db():
    """ì••ì¶•ëœ ë°ì´í„° ë¡œë“œ + ë²¡í„° DB êµ¬ì¶•"""
    
    with gzip.open('law_data.pkl.gz', 'rb') as f:
        all_data = pickle.load(f)
    
    chroma_client = chromadb.Client(Settings(
        anonymized_telemetry=False,
        allow_reset=True
    ))
    
    try:
        chroma_client.delete_collection("osh_law")
    except:
        pass
    
    collection = chroma_client.create_collection(
        name="osh_law",
        metadata={"hnsw:space": "cosine"}
    )
    
    batch_size = 100
    for i in range(0, len(all_data), batch_size):
        batch = all_data[i:i+batch_size]
        
        collection.add(
            documents=[item['full_text'][:1500] for item in batch],
            embeddings=[item['embedding'] for item in batch],
            metadatas=[{
                "type": str(item['type']),
                "law_name": str(item['law_name']),
                "number": str(item['number']),
                "title": str(item.get('title', '')),
                "idx": str(i+j)
            } for j, item in enumerate(batch)],
            ids=[f"item_{i+j}" for j in range(len(batch))]
        )
    
    return all_data, collection

def keyword_search(query, all_data, max_results=5):
    """í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ - ì œëª©ê³¼ ë‚´ìš©ì—ì„œ ê²€ìƒ‰"""
    results = []
    query_lower = query.lower()
    
    # í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
    keywords = []
    if 'ì‘ì—…í™˜ê²½ì¸¡ì •' in query:
        keywords.append('ì‘ì—…í™˜ê²½ì¸¡ì •')
    if 'ì•ˆì „ê´€ë¦¬ì' in query:
        keywords.append('ì•ˆì „ê´€ë¦¬ì')
    if 'ë³´ê±´ê´€ë¦¬ì' in query:
        keywords.append('ë³´ê±´ê´€ë¦¬ì')
    if 'ì•ˆì „ë³´ê±´êµìœ¡' in query or 'êµìœ¡' in query:
        keywords.append('ì•ˆì „ë³´ê±´êµìœ¡')
    if 'ê³¼íƒœë£Œ' in query:
        keywords.append('ê³¼íƒœë£Œ')
    if 'ìœ í•´ì¸ì' in query or 'ëŒ€ìƒë¬¼ì§ˆ' in query:
        keywords.append('ìœ í•´ì¸ì')
    if 'msds' in query_lower or 'ë¬¼ì§ˆì•ˆì „ë³´ê±´ìë£Œ' in query:
        keywords.append('ë¬¼ì§ˆì•ˆì „ë³´ê±´ìë£Œ')
    if 'ìœ„í—˜ë¬¼' in query:
        keywords.append('ìœ„í—˜ë¬¼')
    if 'íŠ¹ìˆ˜ê±´ê°•ì§„ë‹¨' in query or 'ê±´ê°•ì§„ë‹¨' in query:
        keywords.append('íŠ¹ìˆ˜ê±´ê°•ì§„ë‹¨')
    if 'ë…¸ì¶œê¸°ì¤€' in query:
        keywords.append('ë…¸ì¶œê¸°ì¤€')
    if 'í—ˆìš©ê¸°ì¤€' in query:
        keywords.append('í—ˆìš©ê¸°ì¤€')
    if 'ë°°ì¶œí—ˆìš©' in query or 'ë°°ì¶œê¸°ì¤€' in query:
        keywords.append('ë°°ì¶œí—ˆìš©')
    
    # í‚¤ì›Œë“œë¡œ ê²€ìƒ‰
    for idx, item in enumerate(all_data):
        title = item.get('title', '').lower()
        text = item['full_text'][:500].lower()
        
        for kw in keywords:
            if kw.lower() in title or kw.lower() in text:
                results.append({
                    'idx': idx,
                    'item': item,
                    'match': kw
                })
                break
    
    # ë³„í‘œ ìš°ì„  ì •ë ¬
    results.sort(key=lambda x: (0 if x['item']['type'] == 'ë³„í‘œ' else 1))
    
    return results[:max_results]

def search_law(query, collection, embedding_model, n_results=10):
    """ë²¡í„° ê²€ìƒ‰"""
    query_embedding = embedding_model.encode(query).tolist()
    return collection.query(
        query_embeddings=[query_embedding],
        n_results=n_results
    )

def get_full_text(all_data, idx, max_len=8000):
    """ì›ë³¸ ì „ì²´ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°"""
    if 0 <= idx < len(all_data):
        full_text = all_data[idx]['full_text']
        if len(full_text) > max_len:
            return full_text[:max_len] + f"\n\n... (ì „ì²´ {len(full_text)}ì ì¤‘ {max_len}ìë§Œ í‘œì‹œ)"
        return full_text
    return None

def ask_chatbot(question, collection, embedding_model, all_data):
    # 1. í‚¤ì›Œë“œ ê²€ìƒ‰ ë¨¼ì €!
    keyword_results = keyword_search(question, all_data, max_results=3)
    
    # 2. ë²¡í„° ê²€ìƒ‰
    vector_results = search_law(question, collection, embedding_model, n_results=10)
    
    # 3. ê²°ê³¼ ë³‘í•© (í‚¤ì›Œë“œ ê²°ê³¼ ìš°ì„ )
    context_parts = []
    selected_metas = []
    used_idx = set()
    
    # í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ ì¶”ê°€ (ë³„í‘œ ì „ì²´ í…ìŠ¤íŠ¸)
    for r in keyword_results:
        idx = r['idx']
        if idx not in used_idx:
            item = r['item']
            full_text = get_full_text(all_data, idx, max_len=10000)
            if full_text:
                context_parts.append(full_text)
                selected_metas.append({
                    'type': item['type'],
                    'law_name': item['law_name'],
                    'number': item['number'],
                    'title': item.get('title', ''),
                    'idx': str(idx)
                })
                used_idx.add(idx)
    
    # ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ ì¶”ê°€
    for doc, meta in zip(vector_results['documents'][0], vector_results['metadatas'][0]):
        idx = int(meta.get('idx', -1))
        if idx not in used_idx and len(context_parts) < 8:
            max_len = 6000 if meta['type'] == 'ë³„í‘œ' else 2000
            full_text = get_full_text(all_data, idx, max_len=max_len)
            if full_text:
                context_parts.append(full_text)
                selected_metas.append(meta)
                used_idx.add(idx)
    
    context = "\n\n---\n\n".join(context_parts)
    
    client = Anthropic(api_key=ANTHROPIC_API_KEY)
    
    prompt = f"""ë‹¹ì‹ ì€ ì•ˆì „í™˜ê²½ ë²•ê·œ ì „ë¬¸ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤.
ì•„ë˜ ì œê³µëœ ë²•ë ¹ ì¡°ë¬¸ê³¼ ë³„í‘œë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

## ì°¸ê³  ìë£Œ:
{context}

## ì§ˆë¬¸:
{question}

## ë‹µë³€ ì§€ì¹¨:
1. ë°˜ë“œì‹œ ìœ„ ìë£Œ ë‚´ìš©ì„ ê·¼ê±°ë¡œ ë‹µë³€í•˜ì„¸ìš”
2. ì¶œì²˜ë¥¼ ëª…í™•íˆ ë°íˆì„¸ìš” (ì˜ˆ: "ì‚°ì—…ì•ˆì „ë³´ê±´ë²• ì‹œí–‰ê·œì¹™ ë³„í‘œ 21ì— ë”°ë¥´ë©´...")
3. **ë³„í‘œì— ëª©ë¡ì´ë‚˜ ê¸°ì¤€ì´ ìˆìœ¼ë©´ í•´ë‹¹ ë‚´ìš©ì„ ìƒì„¸íˆ ì¸ìš©í•˜ì„¸ìš”**
4. **ë¬¼ì§ˆ ëª©ë¡ì„ ë¬¼ì–´ë³´ë©´ ì „ì²´ ëª©ë¡ì„ ë¹ ì§ì—†ì´ ë‚˜ì—´í•˜ì„¸ìš”**
5. ìë£Œì— ì—†ëŠ” ë‚´ìš©ì€ "í•´ë‹¹ ë‚´ìš©ì€ ì œê³µëœ ìë£Œì—ì„œ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤"ë¼ê³  ë‹µí•˜ì„¸ìš”
6. ì‰½ê³  ì¹œì ˆí•˜ê²Œ ì„¤ëª…í•˜ì„¸ìš”
7. ë§ˆì§€ë§‰ì— ë©´ì±…ì¡°í•­: "â€» ë³¸ ë‹µë³€ì€ ì°¸ê³ ìš©ì´ë©°, ì •í™•í•œ ë²•ë¥  í•´ì„ì€ ì „ë¬¸ê°€ì™€ ìƒë‹´í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤."

## ë‹µë³€:"""

    response = client.messages.create(
        model="claude-sonnet-4-20250514",
        max_tokens=4000,
        messages=[{"role": "user", "content": prompt}]
    )
    
    return_results = {
        'documents': [context_parts],
        'metadatas': [selected_metas]
    }
    
    return response.content[0].text, return_results

# ============================================================
# ë©”ì¸ UI
# ============================================================
st.title("ğŸ­ ì•ˆì „í™˜ê²½ ë²•ê·œ AI ìƒë‹´ì‚¬")
st.markdown("ì‚°ì—…ì•ˆì „, í™”í•™ë¬¼ì§ˆ, í™˜ê²½, ìœ„í—˜ë¬¼ ê´€ë ¨ ë²•ê·œë¥¼ ë¬¼ì–´ë³´ì„¸ìš”!")

# ì‚¬ì´ë“œë°”
with st.sidebar:
    st.header("ğŸ“š í¬í•¨ëœ ë²•ê·œ (33ê°œ)")
    
    with st.expander("ğŸ”§ ì‚°ì—…ì•ˆì „ë³´ê±´ (4ê°œ)"):
        st.markdown("ì‚°ì—…ì•ˆì „ë³´ê±´ë²•, ì‹œí–‰ë ¹, ì‹œí–‰ê·œì¹™, ì•ˆì „ë³´ê±´ê¸°ì¤€ê·œì¹™")
    
    with st.expander("ğŸ§ª í™”í•™ë¬¼ì§ˆ (6ê°œ)"):
        st.markdown("í™”í•™ë¬¼ì§ˆê´€ë¦¬ë²•, í™”í‰ë²• + ê° ì‹œí–‰ë ¹/ê·œì¹™")
    
    with st.expander("ğŸ”¥ ìœ„í—˜ë¬¼/ê³ ì••ê°€ìŠ¤ (6ê°œ)"):
        st.markdown("ìœ„í—˜ë¬¼ì•ˆì „ê´€ë¦¬ë²•, ê³ ì••ê°€ìŠ¤ë²• + ê° ì‹œí–‰ë ¹/ê·œì¹™")
    
    with st.expander("ğŸŒ¿ í™˜ê²½ (9ê°œ)"):
        st.markdown("ëŒ€ê¸°í™˜ê²½ë³´ì „ë²•, ë¬¼í™˜ê²½ë³´ì „ë²•, íê¸°ë¬¼ê´€ë¦¬ë²• + ê° ì‹œí–‰ë ¹/ê·œì¹™")
    
    with st.expander("ğŸ“‹ ê³ ì‹œ/ì§€ì¹¨ (8ê°œ)"):
        st.markdown("ìœ„í—˜ì„±í‰ê°€, MSDS, ë…¸ì¶œê¸°ì¤€, ì‘ì—…í™˜ê²½ì¸¡ì •, ì‚¬ë¬´ì‹¤ê³µê¸°, ìœ„í—˜ë¬¼ì„¸ë¶€, ê³ ì••ê°€ìŠ¤, ìœ í•´í™”í•™ë¬¼ì§ˆì·¨ê¸‰")
    
    st.markdown("---")
    st.header("ğŸ’¡ ì§ˆë¬¸ ì˜ˆì‹œ")
    st.markdown("""
    - ì•ˆì „ê´€ë¦¬ì ì„ ì„ ê¸°ì¤€ì€?
    - **ì‘ì—…í™˜ê²½ì¸¡ì •ëŒ€ìƒë¬¼ì§ˆ ëª©ë¡ì€?**
    - ìœ„í—˜ë¬¼ ì €ì¥ì†Œ ê¸°ì¤€ì€?
    - ì•ˆì „ë³´ê±´êµìœ¡ ì‹œê°„ì€?
    - ê³¼íƒœë£Œ ê¸°ì¤€ì´ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?
    """)
    
    st.markdown("---")
    st.markdown("âš ï¸ ë³¸ ì„œë¹„ìŠ¤ëŠ” ì°¸ê³ ìš©ì…ë‹ˆë‹¤.")
    st.markdown("Made with â¤ï¸ by íìŠ¤")

# ì‹œìŠ¤í…œ ì´ˆê¸°í™”
with st.spinner("ğŸ”„ ì‹œìŠ¤í…œ ì¤€ë¹„ ì¤‘..."):
    embedding_model = load_embedding_model()
    all_data, collection = load_data_and_build_db()

# í†µê³„
article_count = len([d for d in all_data if d['type'] == 'ì¡°ë¬¸'])
table_count = len([d for d in all_data if d['type'] == 'ë³„í‘œ'])
st.success(f"âœ… ì¤€ë¹„ ì™„ë£Œ! (ì¡°ë¬¸ {article_count:,}ê°œ + ë³„í‘œ {table_count:,}ê°œ)")

# ì±„íŒ… íˆìŠ¤í† ë¦¬
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ì‚¬ìš©ì ì…ë ¥
if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
    
    with st.chat_message("assistant"):
        with st.spinner("ğŸ” ê²€ìƒ‰ ì¤‘..."):
            try:
                answer, search_results = ask_chatbot(prompt, collection, embedding_model, all_data)
                st.markdown(answer)
                
                with st.expander("ğŸ“œ ì°¸ê³  ìë£Œ ë³´ê¸°"):
                    for doc, meta in zip(search_results['documents'][0], search_results['metadatas'][0]):
                        badge = "ğŸ“‹" if meta['type'] == 'ì¡°ë¬¸' else "ğŸ“Š"
                        st.markdown(f"**{badge} {meta['law_name']} {meta['number']}** - {meta['title']}")
                        st.text(doc[:800] + "..." if len(doc) > 800 else doc)
                        st.markdown("---")
                
                st.session_state.messages.append({"role": "assistant", "content": answer})
            except Exception as e:
                st.error(f"ì˜¤ë¥˜: {str(e)}")
