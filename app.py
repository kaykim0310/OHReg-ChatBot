"""
ğŸ­ ì•ˆì „í™˜ê²½ ë²•ê·œ AI ìƒë‹´ì‚¬
Streamlit ì›¹ ì•± (ì§„ì§œ ìµœì¢… ì™„ì „ì²´ ë²„ì „)

í¬í•¨ ë²•ë ¹: 25ê°œ
í¬í•¨ ê³ ì‹œ: 8ê°œ
ì´: 33ê°œ ë²•ê·œ
"""

import streamlit as st
import requests
import xml.etree.ElementTree as ET
import chromadb
from sentence_transformers import SentenceTransformer
from anthropic import Anthropic

# ============================================================
# í˜ì´ì§€ ì„¤ì •
# ============================================================
st.set_page_config(
    page_title="ì•ˆì „í™˜ê²½ ë²•ê·œ AI ìƒë‹´ì‚¬",
    page_icon="ğŸ­",
    layout="centered"
)

# ============================================================
# ğŸ” API í‚¤ ì„¤ì •
# ============================================================
ANTHROPIC_API_KEY = st.secrets["ANTHROPIC_API_KEY"]

# ============================================================
# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
# ============================================================
if 'messages' not in st.session_state:
    st.session_state.messages = []

# ============================================================
# ğŸ“š ë²•ë ¹/ê³ ì‹œ ëª©ë¡ ì •ì˜
# ============================================================

# ë²•ë ¹ (target: law)
LAWS = [
    # ========== ì‚°ì—…ì•ˆì „ë³´ê±´ ==========
    ("276853", "ì‚°ì—…ì•ˆì „ë³´ê±´ë²•"),
    ("277411", "ì‚°ì—…ì•ˆì „ë³´ê±´ë²• ì‹œí–‰ë ¹"),
    ("271485", "ì‚°ì—…ì•ˆì „ë³´ê±´ë²• ì‹œí–‰ê·œì¹™"),
    ("277059", "ì‚°ì—…ì•ˆì „ë³´ê±´ê¸°ì¤€ì— ê´€í•œ ê·œì¹™"),
    
    # ========== í™”í•™ë¬¼ì§ˆê´€ë¦¬ë²• (í™”ê´€ë²•) ==========
    ("276815", "í™”í•™ë¬¼ì§ˆê´€ë¦¬ë²•"),
    ("280507", "í™”í•™ë¬¼ì§ˆê´€ë¦¬ë²• ì‹œí–‰ë ¹"),
    ("279031", "í™”í•™ë¬¼ì§ˆê´€ë¦¬ë²• ì‹œí–‰ê·œì¹™"),
    
    # ========== í™”í‰ë²• ==========
    ("279805", "í™”í•™ë¬¼ì§ˆì˜ ë“±ë¡ ë° í‰ê°€ ë“±ì— ê´€í•œ ë²•ë¥ "),
    ("280633", "í™”í•™ë¬¼ì§ˆì˜ ë“±ë¡ ë° í‰ê°€ ë“±ì— ê´€í•œ ë²•ë¥  ì‹œí–‰ë ¹"),
    ("282061", "í™”í•™ë¬¼ì§ˆì˜ ë“±ë¡ ë° í‰ê°€ ë“±ì— ê´€í•œ ë²•ë¥  ì‹œí–‰ê·œì¹™"),
    
    # ========== ìœ„í—˜ë¬¼ì•ˆì „ê´€ë¦¬ë²• ==========
    ("259933", "ìœ„í—˜ë¬¼ì•ˆì „ê´€ë¦¬ë²•"),
    ("273077", "ìœ„í—˜ë¬¼ì•ˆì „ê´€ë¦¬ë²• ì‹œí–‰ë ¹"),
    ("262765", "ìœ„í—˜ë¬¼ì•ˆì „ê´€ë¦¬ë²• ì‹œí–‰ê·œì¹™"),
    
    # ========== ê³ ì••ê°€ìŠ¤ ì•ˆì „ê´€ë¦¬ë²• ==========
    ("276461", "ê³ ì••ê°€ìŠ¤ ì•ˆì „ê´€ë¦¬ë²•"),
    ("278293", "ê³ ì••ê°€ìŠ¤ ì•ˆì „ê´€ë¦¬ë²• ì‹œí–‰ë ¹"),
    ("278693", "ê³ ì••ê°€ìŠ¤ ì•ˆì „ê´€ë¦¬ë²• ì‹œí–‰ê·œì¹™"),
    
    # ========== ëŒ€ê¸°í™˜ê²½ë³´ì „ë²• ==========
    ("279785", "ëŒ€ê¸°í™˜ê²½ë³´ì „ë²•"),
    ("280555", "ëŒ€ê¸°í™˜ê²½ë³´ì „ë²• ì‹œí–‰ë ¹"),
    ("280747", "ëŒ€ê¸°í™˜ê²½ë³´ì „ë²• ì‹œí–‰ê·œì¹™"),
    
    # ========== ë¬¼í™˜ê²½ë³´ì „ë²• ==========
    ("276739", "ë¬¼í™˜ê²½ë³´ì „ë²•"),
    ("281847", "ë¬¼í™˜ê²½ë³´ì „ë²• ì‹œí–‰ë ¹"),
    ("282047", "ë¬¼í™˜ê²½ë³´ì „ë²• ì‹œí–‰ê·œì¹™"),
    
    # ========== íê¸°ë¬¼ê´€ë¦¬ë²• ==========
    ("279797", "íê¸°ë¬¼ê´€ë¦¬ë²•"),
    ("282339", "íê¸°ë¬¼ê´€ë¦¬ë²• ì‹œí–‰ë ¹"),
    ("282261", "íê¸°ë¬¼ê´€ë¦¬ë²• ì‹œí–‰ê·œì¹™"),
]

# í–‰ì •ê·œì¹™/ê³ ì‹œ (target: admrul)
ADMIN_RULES = [
    # ========== ì‚°ì—…ì•ˆì „ë³´ê±´ ê³ ì‹œ ==========
    ("2100000251014", "ì‚¬ì—…ì¥ ìœ„í—˜ì„±í‰ê°€ì— ê´€í•œ ì§€ì¹¨"),
    ("2100000262720", "í™”í•™ë¬¼ì§ˆì˜ ë¶„ë¥˜Â·í‘œì‹œ ë° ë¬¼ì§ˆì•ˆì „ë³´ê±´ìë£Œì— ê´€í•œ ê¸°ì¤€"),
    ("2100000186058", "í™”í•™ë¬¼ì§ˆ ë° ë¬¼ë¦¬ì  ì¸ìì˜ ë…¸ì¶œê¸°ì¤€"),
    ("2100000186111", "ì‘ì—…í™˜ê²½ì¸¡ì • ë° ì •ë„ê´€ë¦¬ ë“±ì— ê´€í•œ ê³ ì‹œ"),
    ("2100000186112", "ì‚¬ë¬´ì‹¤ ê³µê¸°ê´€ë¦¬ ì§€ì¹¨"),
    
    # ========== ìœ„í—˜ë¬¼ ê³ ì‹œ ==========
    ("2100000249286", "ìœ„í—˜ë¬¼ì•ˆì „ê´€ë¦¬ì— ê´€í•œ ì„¸ë¶€ê¸°ì¤€"),
    
    # ========== ê³ ì••ê°€ìŠ¤ ê³ ì‹œ ==========
    ("2100000211965", "ê³ ì••ê°€ìŠ¤ì•ˆì „ê´€ë¦¬ê¸°ì¤€í†µí•©ê³ ì‹œ"),
    
    # ========== í™”í•™ë¬¼ì§ˆ ê³ ì‹œ ==========
    ("2100000262588", "ìœ í•´í™”í•™ë¬¼ì§ˆë³„ êµ¬ì²´ì ì¸ ì·¨ê¸‰ê¸°ì¤€ì— ê´€í•œ ê·œì •"),
]

# ============================================================
# ë²•ë ¹ ë°ì´í„° ë¡œë“œ í•¨ìˆ˜ë“¤
# ============================================================
@st.cache_resource
def load_embedding_model():
    """ì„ë² ë”© ëª¨ë¸ ë¡œë“œ"""
    return SentenceTransformer('jhgan/ko-sroberta-multitask')

@st.cache_data
def get_law_data(law_msn, law_name, target="law", oc="kangyoon.kim"):
    """ë²•ë ¹/ê³ ì‹œ ì¡°ë¬¸ + ë³„í‘œ ê°€ì ¸ì˜¤ê¸°"""
    
    if target == "law":
        url = "http://www.law.go.kr/DRF/lawService.do"
    else:
        url = "http://www.law.go.kr/DRF/admRulService.do"
    
    params = {
        "OC": oc,
        "target": target,
        "type": "XML",
        "MST": law_msn
    }
    
    try:
        response = requests.get(url, params=params, timeout=30)
        response.encoding = 'utf-8'
        root = ET.fromstring(response.content)
    except Exception as e:
        return []
    
    all_data = []
    
    # 1. ì¡°ë¬¸ ê°€ì ¸ì˜¤ê¸°
    for article in root.findall('.//ì¡°ë¬¸ë‹¨ìœ„'):
        article_no = article.findtext('ì¡°ë¬¸ë²ˆí˜¸', '')
        article_title = article.findtext('ì¡°ë¬¸ì œëª©', '')
        article_content = article.findtext('ì¡°ë¬¸ë‚´ìš©', '')
        
        hang_list = []
        for hang in article.findall('.//í•­'):
            hang_content = hang.findtext('í•­ë‚´ìš©', '')
            if hang_content:
                hang_list.append(hang_content)
        
        full_text = f"[{law_name}] ì œ{article_no}ì¡°"
        if article_title:
            full_text += f"({article_title})"
        full_text += "\n"
        if article_content:
            full_text += article_content + "\n"
        if hang_list:
            full_text += "\n".join(hang_list)
        
        if article_content or hang_list:
            all_data.append({
                "type": "ì¡°ë¬¸",
                "law_name": law_name,
                "number": f"ì œ{article_no}ì¡°",
                "title": article_title or "",
                "full_text": full_text.strip()
            })
    
    # 2. ë³„í‘œ ê°€ì ¸ì˜¤ê¸°
    for bt in root.findall('.//ë³„í‘œë‹¨ìœ„'):
        bt_no = bt.findtext('ë³„í‘œë²ˆí˜¸', '')
        bt_title = bt.findtext('ë³„í‘œì œëª©', '')
        bt_content = bt.findtext('ë³„í‘œë‚´ìš©', '')
        
        if bt_content and len(bt_content) > 50:
            full_text = f"[{law_name}] [ë³„í‘œ {bt_no}] {bt_title}\n\n{bt_content}"
            
            all_data.append({
                "type": "ë³„í‘œ",
                "law_name": law_name,
                "number": f"ë³„í‘œ {bt_no}",
                "title": bt_title or "",
                "full_text": full_text.strip()
            })
    
    return all_data

@st.cache_data
def get_all_data():
    """ëª¨ë“  ë²•ë ¹/ê³ ì‹œ ë°ì´í„° í†µí•©"""
    all_data = []
    
    progress_text = st.empty()
    progress_bar = st.progress(0)
    total = len(LAWS) + len(ADMIN_RULES)
    
    # ë²•ë ¹ ë¡œë“œ
    for i, (msn, name) in enumerate(LAWS):
        progress_text.text(f"ğŸ“¥ {name} ë¡œë“œ ì¤‘...")
        progress_bar.progress((i + 1) / total)
        data = get_law_data(msn, name, target="law")
        all_data.extend(data)
    
    # í–‰ì •ê·œì¹™/ê³ ì‹œ ë¡œë“œ
    for i, (msn, name) in enumerate(ADMIN_RULES):
        progress_text.text(f"ğŸ“¥ {name} ë¡œë“œ ì¤‘...")
        progress_bar.progress((len(LAWS) + i + 1) / total)
        data = get_law_data(msn, name, target="admrul")
        all_data.extend(data)
    
    progress_text.empty()
    progress_bar.empty()
    return all_data

@st.cache_resource
def build_vector_db(_embedding_model, all_data):
    """ë²¡í„° DB êµ¬ì¶•"""
    chroma_client = chromadb.Client()
    
    try:
        chroma_client.delete_collection("osh_law")
    except:
        pass
    
    collection = chroma_client.create_collection(name="osh_law")
    
    for idx, item in enumerate(all_data):
        text = item['full_text']
        
        if len(text) > 2000:
            text = text[:2000]
        
        embedding = _embedding_model.encode(text).tolist()
        
        collection.add(
            documents=[text],
            embeddings=[embedding],
            metadatas=[{
                "type": item['type'],
                "law_name": item['law_name'],
                "number": item['number'],
                "title": item['title']
            }],
            ids=[f"item_{idx}"]
        )
    
    return collection

def search_law(query, collection, embedding_model, n_results=5):
    """ê´€ë ¨ ì¡°ë¬¸/ë³„í‘œ ê²€ìƒ‰"""
    query_embedding = embedding_model.encode(query).tolist()
    
    results = collection.query(
        query_embeddings=[query_embedding],
        n_results=n_results
    )
    
    return results

def ask_chatbot(question, collection, embedding_model):
    """ì±—ë´‡ ì§ˆë¬¸-ë‹µë³€"""
    search_results = search_law(question, collection, embedding_model)
    context = "\n\n---\n\n".join(search_results['documents'][0])
    
    client = Anthropic(api_key=ANTHROPIC_API_KEY)
    
    prompt = f"""ë‹¹ì‹ ì€ ì•ˆì „í™˜ê²½ ë²•ê·œ ì „ë¬¸ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤.
ì•„ë˜ ì œê³µëœ ë²•ë ¹, ê³ ì‹œ, ë³„í‘œë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

## ì°¸ê³  ìë£Œ:
{context}

## ì§ˆë¬¸:
{question}

## ë‹µë³€ ì§€ì¹¨:
1. ë°˜ë“œì‹œ ìœ„ ìë£Œ ë‚´ìš©ì„ ê·¼ê±°ë¡œ ë‹µë³€í•˜ì„¸ìš”
2. ì¶œì²˜ë¥¼ ëª…í™•íˆ ë°íˆì„¸ìš” (ì˜ˆ: "ì‚°ì—…ì•ˆì „ë³´ê±´ë²• ì œ29ì¡°ì— ë”°ë¥´ë©´...", "ìœ„í—˜ë¬¼ì•ˆì „ê´€ë¦¬ì— ê´€í•œ ì„¸ë¶€ê¸°ì¤€ ì œ5ì¡°ì— ë”°ë¥´ë©´...")
3. ìë£Œì— ì—†ëŠ” ë‚´ìš©ì€ "í•´ë‹¹ ë‚´ìš©ì€ ì œê³µëœ ìë£Œì—ì„œ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤"ë¼ê³  ë‹µí•˜ì„¸ìš”
4. ì‰½ê³  ì¹œì ˆí•˜ê²Œ ì„¤ëª…í•˜ì„¸ìš”
5. ë§ˆì§€ë§‰ì— ë©´ì±…ì¡°í•­: "â€» ë³¸ ë‹µë³€ì€ ì°¸ê³ ìš©ì´ë©°, ì •í™•í•œ ë²•ë¥  í•´ì„ì€ ì „ë¬¸ê°€ì™€ ìƒë‹´í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤."

## ë‹µë³€:"""

    response = client.messages.create(
        model="claude-sonnet-4-20250514",
        max_tokens=2000,
        messages=[{"role": "user", "content": prompt}]
    )
    
    return response.content[0].text, search_results

# ============================================================
# ë©”ì¸ UI
# ============================================================
st.title("ğŸ­ ì•ˆì „í™˜ê²½ ë²•ê·œ AI ìƒë‹´ì‚¬")
st.markdown("ì‚°ì—…ì•ˆì „, í™”í•™ë¬¼ì§ˆ, í™˜ê²½, ìœ„í—˜ë¬¼ ê´€ë ¨ ë²•ê·œë¥¼ ë¬¼ì–´ë³´ì„¸ìš”!")

# ì‚¬ì´ë“œë°”
with st.sidebar:
    st.header("ğŸ“š í¬í•¨ëœ ë²•ê·œ (33ê°œ)")
    
    with st.expander("ğŸ”§ ì‚°ì—…ì•ˆì „ë³´ê±´ (4ê°œ ë²•ë ¹)", expanded=False):
        st.markdown("""
        - ì‚°ì—…ì•ˆì „ë³´ê±´ë²•
        - ì‹œí–‰ë ¹ / ì‹œí–‰ê·œì¹™
        - ì‚°ì—…ì•ˆì „ë³´ê±´ê¸°ì¤€ì— ê´€í•œ ê·œì¹™
        """)
    
    with st.expander("ğŸ§ª í™”í•™ë¬¼ì§ˆ (6ê°œ ë²•ë ¹)", expanded=False):
        st.markdown("""
        - í™”í•™ë¬¼ì§ˆê´€ë¦¬ë²• (í™”ê´€ë²•)
        - í™”í‰ë²•
        - ê° ì‹œí–‰ë ¹/ì‹œí–‰ê·œì¹™
        """)
    
    with st.expander("ğŸ”¥ ìœ„í—˜ë¬¼/ê³ ì••ê°€ìŠ¤ (6ê°œ ë²•ë ¹)", expanded=False):
        st.markdown("""
        - ìœ„í—˜ë¬¼ì•ˆì „ê´€ë¦¬ë²•
        - ê³ ì••ê°€ìŠ¤ ì•ˆì „ê´€ë¦¬ë²•
        - ê° ì‹œí–‰ë ¹/ì‹œí–‰ê·œì¹™
        """)
    
    with st.expander("ğŸŒ¿ í™˜ê²½ (9ê°œ ë²•ë ¹)", expanded=False):
        st.markdown("""
        - ëŒ€ê¸°í™˜ê²½ë³´ì „ë²•
        - ë¬¼í™˜ê²½ë³´ì „ë²•
        - íê¸°ë¬¼ê´€ë¦¬ë²•
        - ê° ì‹œí–‰ë ¹/ì‹œí–‰ê·œì¹™
        """)
    
    with st.expander("ğŸ“‹ ê³ ì‹œ/ì§€ì¹¨ (8ê°œ)", expanded=False):
        st.markdown("""
        **ì‚°ì•ˆë²• ê´€ë ¨**
        - ìœ„í—˜ì„±í‰ê°€ ì§€ì¹¨
        - MSDS ê¸°ì¤€
        - ë…¸ì¶œê¸°ì¤€
        - ì‘ì—…í™˜ê²½ì¸¡ì • ê³ ì‹œ
        - ì‚¬ë¬´ì‹¤ ê³µê¸°ê´€ë¦¬ ì§€ì¹¨
        
        **ê¸°íƒ€**
        - ìœ„í—˜ë¬¼ì•ˆì „ê´€ë¦¬ ì„¸ë¶€ê¸°ì¤€
        - ê³ ì••ê°€ìŠ¤ì•ˆì „ê´€ë¦¬ê¸°ì¤€í†µí•©ê³ ì‹œ
        - ìœ í•´í™”í•™ë¬¼ì§ˆ ì·¨ê¸‰ê¸°ì¤€
        """)
    
    st.markdown("---")
    
    st.header("ğŸ’¡ ì§ˆë¬¸ ì˜ˆì‹œ")
    st.markdown("""
    **ì‚°ì—…ì•ˆì „**
    - ì•ˆì „ê´€ë¦¬ì ì„ ì„ ê¸°ì¤€ì€?
    - ìœ„í—˜ì„±í‰ê°€ ì ˆì°¨ëŠ”?
    
    **í™”í•™ë¬¼ì§ˆ**
    - MSDS ì‘ì„± ê¸°ì¤€ì€?
    - ìœ í•´í™”í•™ë¬¼ì§ˆ ì·¨ê¸‰ê¸°ì¤€ì€?
    
    **ìœ„í—˜ë¬¼**
    - ìœ„í—˜ë¬¼ ì €ì¥ì†Œ ê¸°ì¤€ì€?
    - ìœ„í—˜ë¬¼ ì•ˆì „ê±°ë¦¬ëŠ”?
    
    **í™˜ê²½**
    - ëŒ€ê¸°ë°°ì¶œí—ˆìš©ê¸°ì¤€ì€?
    - íê¸°ë¬¼ ì²˜ë¦¬ ê¸°ì¤€ì€?
    """)
    
    st.markdown("---")
    
    st.markdown("""
    **âš ï¸ ë©´ì±…ì¡°í•­**  
    ë³¸ ì„œë¹„ìŠ¤ëŠ” ì°¸ê³ ìš©ì´ë©°, 
    ë²•ë¥ ì  íš¨ë ¥ì´ ì—†ìŠµë‹ˆë‹¤.
    """)
    
    st.markdown("---")
    st.markdown("Made with â¤ï¸ by íìŠ¤")

# ì‹œìŠ¤í…œ ì´ˆê¸°í™”
with st.spinner("ğŸ”„ ì‹œìŠ¤í…œ ì¤€ë¹„ ì¤‘..."):
    embedding_model = load_embedding_model()
    all_data = get_all_data()
    collection = build_vector_db(embedding_model, all_data)

# í†µê³„ í‘œì‹œ
article_count = len([d for d in all_data if d['type'] == 'ì¡°ë¬¸'])
table_count = len([d for d in all_data if d['type'] == 'ë³„í‘œ'])
law_count = len(LAWS) + len(ADMIN_RULES)
st.success(f"âœ… ì¤€ë¹„ ì™„ë£Œ! ({law_count}ê°œ ë²•ê·œ | ì¡°ë¬¸ {article_count:,}ê°œ + ë³„í‘œ {table_count}ê°œ)")

# ì±„íŒ… íˆìŠ¤í† ë¦¬ í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ì‚¬ìš©ì ì…ë ¥
if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
    
    with st.chat_message("assistant"):
        with st.spinner("ğŸ” ê´€ë ¨ ë²•ê·œ ê²€ìƒ‰ ì¤‘..."):
            try:
                answer, search_results = ask_chatbot(prompt, collection, embedding_model)
                st.markdown(answer)
                
                with st.expander("ğŸ“œ ì°¸ê³ í•œ ë²•ê·œ ìë£Œ ë³´ê¸°"):
                    for i, (doc, meta) in enumerate(zip(
                        search_results['documents'][0], 
                        search_results['metadatas'][0]
                    ), 1):
                        badge = "ğŸ“‹" if meta['type'] == 'ì¡°ë¬¸' else "ğŸ“Š"
                        st.markdown(f"**{badge} {meta['law_name']} {meta['number']}** - {meta['title']}")
                        st.text(doc[:600] + "..." if len(doc) > 600 else doc)
                        st.markdown("---")
                
                st.session_state.messages.append({"role": "assistant", "content": answer})
                
            except Exception as e:
                st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
